{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzoFYn1aexDn",
    "outputId": "451b9fc0-7891-4ae1-c356-86ea111c0009",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\.conda\\envs\\mt_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset (ai4bharat/samanantar, hi)... (this can take a minute)\n",
      "Subset sizes: 800000 100000 100000\n",
      "Loading checkpoint: checkpoint_gmsc.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\AppData\\Local\\Temp\\ipykernel_9608\\1611264263.py:248: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed from epoch 21\n",
      "Training finished.\n",
      "Loading best model for evaluation: best_model_gmsc.pt\n",
      "Greedy metrics already exist — skipping evaluation.\n",
      "\n",
      "✅ Greedy Decode results (metrics_gmsc_greedy.pt):\n",
      "  timestamp: 2025-10-27 17:02:48\n",
      "  BLEU: 19.53\n",
      "  TER: 71.66\n",
      "  METEOR: 39.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\AppData\\Local\\Temp\\ipykernel_9608\\1611264263.py:492: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))\n",
      "C:\\Users\\Ayush\\AppData\\Local\\Temp\\ipykernel_9608\\1611264263.py:503: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  metrics = torch.load(path)\n",
      "C:\\Users\\Ayush\\.conda\\envs\\mt_env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "C:\\Users\\Ayush\\.conda\\envs\\mt_env\\lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample translations to sample_translations_gmsc.json\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Full Colab-ready script: Transformer + Gated Multi-Scale CNN (GMSCNN)\n",
    "# Save as main_gmsc_transformer.py or paste into a Colab cell and run.\n",
    "# Assumes spm_en.model and spm_hi.model exist in working directory OR will train on train split.\n",
    "\n",
    "# =====================\n",
    "# Install & Imports\n",
    "# =====================\n",
    "!pip install -q datasets\n",
    "\n",
    "!pip install -q datasets sentencepiece sacrebleu nltk torch torchvision torchaudio tqdm\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# =====================\n",
    "# Config (editable)\n",
    "# =====================\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "VOCAB_SIZE = 32000           # change as needed\n",
    "BATCH_SIZE = 64              # adjust to GPU memory\n",
    "MAX_LEN = 64\n",
    "MAX_GEN_LEN = 64\n",
    "EPOCHS = 20\n",
    "CLIP = 1.0\n",
    "LEARNING_RATE = 3e-4\n",
    "BEAM_SIZE = 5\n",
    "LENGTH_PENALTY = 0.6\n",
    "\n",
    "# File paths\n",
    "SP_EN_PATH = Path(\"spm_en.model\")\n",
    "SP_HI_PATH = Path(\"spm_hi.model\")\n",
    "BEST_MODEL_PATH = Path(\"best_model_gmsc.pt\")\n",
    "CHECKPOINT_PATH = Path(\"checkpoint_gmsc.pt\")\n",
    "METRICS_PATH = Path(\"metrics_gmsc.pt\")    # final metrics saved\n",
    "PARTIAL_PATH = Path(\"metrics_gmsc.partial.pt\")\n",
    "\n",
    "# Other options\n",
    "USE_LABEL_SMOOTHING = True\n",
    "LABEL_SMOOTHING = 0.1\n",
    "SAVE_EVERY = 10000   # for incremental saving during evaluation (if used)\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# =====================\n",
    "# Dataset: load samanantar (English-Hindi)\n",
    "# =====================\n",
    "print(\"Loading dataset (ai4bharat/samanantar, hi)... (this can take a minute)\")\n",
    "full_dataset = load_dataset(\"ai4bharat/samanantar\", \"hi\", split=\"train\")\n",
    "full_dataset = full_dataset.shuffle(seed=SEED)\n",
    "\n",
    "NUM_EXAMPLES = min(1_000_000, len(full_dataset))  # reduce if resource-limited; set to 1_000_000 if desired\n",
    "subset = full_dataset.select(range(NUM_EXAMPLES))\n",
    "\n",
    "train_end = int(0.8 * len(subset))\n",
    "val_end = int(0.9 * len(subset))\n",
    "train_data = subset.select(range(0, train_end))\n",
    "val_data = subset.select(range(train_end, val_end))\n",
    "test_data = subset.select(range(val_end, len(subset)))\n",
    "print(\"Subset sizes:\", len(train_data), len(val_data), len(test_data))\n",
    "\n",
    "# =====================\n",
    "# Train SentencePiece if missing\n",
    "# =====================\n",
    "def write_lines(dataset_split, src_path, tgt_path):\n",
    "    with open(src_path, \"w\", encoding=\"utf-8\") as sf, open(tgt_path, \"w\", encoding=\"utf-8\") as tf:\n",
    "        for ex in dataset_split:\n",
    "            sf.write(ex[\"src\"].strip().lower() + \"\\n\")\n",
    "            tf.write(ex[\"tgt\"].strip() + \"\\n\")\n",
    "\n",
    "if not SP_EN_PATH.exists() or not SP_HI_PATH.exists():\n",
    "    print(\"Creating train.en / train.hi for SentencePiece training...\")\n",
    "    write_lines(train_data, \"train.en\", \"train.hi\")\n",
    "    print(\"Training SentencePiece models (this may take a while)...\")\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f\"--input=train.en --model_prefix=spm_en --vocab_size={VOCAB_SIZE} --character_coverage=1.0 --model_type=unigram\"\n",
    "    )\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f\"--input=train.hi --model_prefix=spm_hi --vocab_size={VOCAB_SIZE} --character_coverage=0.9995 --model_type=unigram\"\n",
    "    )\n",
    "\n",
    "sp_en = spm.SentencePieceProcessor()\n",
    "sp_hi = spm.SentencePieceProcessor()\n",
    "sp_en.load(str(SP_EN_PATH))\n",
    "sp_hi.load(str(SP_HI_PATH))\n",
    "\n",
    "PAD_EN, BOS_EN, EOS_EN = 0, 1, 2\n",
    "PAD_HI, BOS_HI, EOS_HI = 0, 1, 2\n",
    "\n",
    "# =====================\n",
    "# Dataset & DataLoader\n",
    "# =====================\n",
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, dataset, src_sp, tgt_sp, max_len=MAX_LEN):\n",
    "        self.dataset = dataset\n",
    "        self.src_sp = src_sp\n",
    "        self.tgt_sp = tgt_sp\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.dataset[idx][\"src\"].lower()\n",
    "        tgt_text = self.dataset[idx][\"tgt\"]\n",
    "        src_ids = [BOS_EN] + self.src_sp.encode(src_text)[:self.max_len-2] + [EOS_EN]\n",
    "        tgt_ids = [BOS_HI] + self.tgt_sp.encode(tgt_text)[:self.max_len-2] + [EOS_HI]\n",
    "        # pad\n",
    "        src_ids += [PAD_EN] * (self.max_len - len(src_ids))\n",
    "        tgt_ids += [PAD_HI] * (self.max_len - len(tgt_ids))\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "def get_loader(dataset_split, shuffle=True):\n",
    "    return DataLoader(NMTDataset(dataset_split, sp_en, sp_hi),\n",
    "                      batch_size=BATCH_SIZE, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
    "\n",
    "train_loader = get_loader(train_data, shuffle=True)\n",
    "val_loader = get_loader(val_data, shuffle=False)\n",
    "test_loader = get_loader(test_data, shuffle=False)\n",
    "\n",
    "# =====================\n",
    "# Masks utilities\n",
    "# =====================\n",
    "def create_padding_mask(seq, lang='en'):\n",
    "    pad_id = PAD_EN if lang == 'en' else PAD_HI\n",
    "    return (seq == pad_id)\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    # returns float mask with -inf above diagonal on device\n",
    "    m = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "    return m.to(DEVICE)\n",
    "\n",
    "# =====================\n",
    "# Gated Multi-Scale CNN\n",
    "# =====================\n",
    "class GatedMultiScaleCNN(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.conv3 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(embed_dim, embed_dim, kernel_size=5, padding=2)\n",
    "        self.conv7 = nn.Conv1d(embed_dim, embed_dim, kernel_size=7, padding=3)\n",
    "        self.gate_proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim // 2, 3)\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        residual = x\n",
    "        b, s, d = x.size()\n",
    "        x_t = x.transpose(1, 2)  # (b, d, s)\n",
    "        o3 = self.conv3(x_t).transpose(1, 2)  # (b, s, d)\n",
    "        o5 = self.conv5(x_t).transpose(1, 2)\n",
    "        o7 = self.conv7(x_t).transpose(1, 2)\n",
    "        stacked = torch.stack([o3, o5, o7], dim=-1)  # (b, s, d, 3)\n",
    "        gates = self.gate_proj(residual)             # (b, s, 3)\n",
    "        gates = F.softmax(gates, dim=-1).unsqueeze(2)  # (b, s, 1, 3)\n",
    "        fused = (stacked * gates).sum(-1)            # (b, s, d)\n",
    "        fused = self.activation(fused)\n",
    "        out = self.norm(fused + residual)\n",
    "        return out\n",
    "\n",
    "# =====================\n",
    "# Hybrid Transformer Model (GMSCNN encoder)\n",
    "# =====================\n",
    "class HybridTransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, d_model=512, nhead=8,\n",
    "                 num_layers=3, dim_feedforward=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab, d_model, padding_idx=PAD_EN)\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab, d_model, padding_idx=PAD_HI)\n",
    "        self.pos_enc = nn.Parameter(torch.zeros(1, MAX_LEN, d_model))\n",
    "        self.cnn_encoder = GatedMultiScaleCNN(d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_layers, num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab)\n",
    "\n",
    "    def encode(self, src, src_key_padding_mask):\n",
    "        src_emb = self.src_emb(src) + self.pos_enc[:, :src.size(1), :]\n",
    "        src_cnn = self.cnn_encoder(src_emb)\n",
    "        return self.transformer.encoder(src_cnn, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask, memory_key_padding_mask, tgt_key_padding_mask):\n",
    "        tgt_emb = self.tgt_emb(tgt) + self.pos_enc[:, :tgt.size(1), :]\n",
    "        return self.transformer.decoder(\n",
    "            tgt_emb, memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None,\n",
    "                memory_key_padding_mask=None, tgt_mask=None):\n",
    "        memory = self.encode(src, src_key_padding_mask)\n",
    "        output = self.decode(tgt, memory, tgt_mask, memory_key_padding_mask, tgt_key_padding_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# =====================\n",
    "# Initialize model, loss, optimizer, scheduler\n",
    "# =====================\n",
    "model = HybridTransformerModel(len(sp_en), len(sp_hi), d_model=512, nhead=8, num_layers=3).to(DEVICE)\n",
    "if USE_LABEL_SMOOTHING:\n",
    "    # CrossEntropy with label smoothing is available in PyTorch 1.10+ via nn.CrossEntropyLoss(label_smoothing=...)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_HI, label_smoothing=LABEL_SMOOTHING)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_HI)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE,\n",
    "                                          steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# Resume checkpoint if exists\n",
    "start_epoch = 1\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "PATIENCE = 5\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    print(\"Loading checkpoint:\", CHECKPOINT_PATH)\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(ckpt['scaler_state_dict'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    best_val_loss = ckpt.get('best_val_loss', best_val_loss)\n",
    "    epochs_no_improve = ckpt.get('epochs_no_improve', 0)\n",
    "    print(\"Resumed from epoch\", start_epoch)\n",
    "\n",
    "# =====================\n",
    "# Greedy & Beam decoding utilities\n",
    "# =====================\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model, src_sentence_ids, max_len=MAX_GEN_LEN):\n",
    "    # src_sentence_ids: tensor (1, seq_len)\n",
    "    model.eval()\n",
    "    src = src_sentence_ids.to(DEVICE)\n",
    "    src_mask = create_padding_mask(src, 'en')\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.tensor([[BOS_HI]], dtype=torch.long, device=DEVICE)\n",
    "    for i in range(max_len):\n",
    "        tgt_mask = generate_square_subsequent_mask(ys.size(1)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask, src_mask, create_padding_mask(ys, 'hi'))\n",
    "        logits = model.fc_out(out[:, -1, :])\n",
    "        next_word = logits.argmax(-1).item()\n",
    "        ys = torch.cat([ys, torch.tensor([[next_word]], device=DEVICE)], dim=1)\n",
    "        if next_word == EOS_HI:\n",
    "            break\n",
    "    tokens = [t for t in ys.squeeze().tolist() if t not in [BOS_HI, EOS_HI, PAD_HI]]\n",
    "    text = sp_hi.decode(tokens) if tokens else \"\"\n",
    "    return text.strip()\n",
    "\n",
    "# Simple beam search (length-penalty)\n",
    "class Beam:\n",
    "    def __init__(self, tokens, logprob, state=None):\n",
    "        self.tokens = tokens\n",
    "        self.logprob = logprob\n",
    "        self.state = state\n",
    "\n",
    "def beam_decode(model, src_sentence_ids, beam_size=BEAM_SIZE, max_len=MAX_GEN_LEN, length_penalty=LENGTH_PENALTY):\n",
    "    model.eval()\n",
    "    src = src_sentence_ids.to(DEVICE)\n",
    "    src_mask = create_padding_mask(src, 'en')\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    # initial beam\n",
    "    beams = [Beam(tokens=[BOS_HI], logprob=0.0)]\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for beam in beams:\n",
    "            if beam.tokens[-1] == EOS_HI:\n",
    "                all_candidates.append(beam)\n",
    "                continue\n",
    "            tgt = torch.tensor([beam.tokens], dtype=torch.long, device=DEVICE)\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt.size(1)).to(DEVICE)\n",
    "            out = model.decode(tgt, memory, tgt_mask, src_mask, create_padding_mask(tgt, 'hi'))\n",
    "            logits = model.fc_out(out[:, -1, :])  # (1, vocab)\n",
    "            log_probs = F.log_softmax(logits, dim=-1).squeeze(0)  # (vocab,)\n",
    "\n",
    "            topk = torch.topk(log_probs, beam_size)\n",
    "            for k in range(beam_size):\n",
    "                token = int(topk.indices[k].item())\n",
    "                lp = float(topk.values[k].item())\n",
    "                new_beam = Beam(tokens=beam.tokens + [token], logprob=beam.logprob + lp)\n",
    "                all_candidates.append(new_beam)\n",
    "\n",
    "        # select top beams\n",
    "        beams = sorted(all_candidates, key=lambda b: b.logprob / ((5 + len(b.tokens)) ** length_penalty), reverse=True)[:beam_size]\n",
    "\n",
    "        # stop if all beams ended\n",
    "        if all([b.tokens[-1] == EOS_HI for b in beams]):\n",
    "            break\n",
    "\n",
    "    # choose best completed or best beam\n",
    "    best = max(beams, key=lambda b: b.logprob / ((5 + len(b.tokens)) ** length_penalty))\n",
    "    tokens = [t for t in best.tokens if t not in [BOS_HI, EOS_HI, PAD_HI]]\n",
    "    return sp_hi.decode(tokens) if tokens else \"\"\n",
    "\n",
    "# =====================\n",
    "# Evaluation utilities (BLEU, METEOR, TER) with incremental save\n",
    "# =====================\n",
    "def evaluate_and_save(model, dataset_split, use_beam=False, metrics_path=METRICS_PATH, partial_path=PARTIAL_PATH, save_every=SAVE_EVERY):\n",
    "    # Load partial results if exist\n",
    "    if partial_path.exists():\n",
    "        print(\"Loading partial results:\", partial_path)\n",
    "        data = torch.load(partial_path)\n",
    "        refs, hyps, meteor_scores, start_idx = data[\"refs\"], data[\"hyps\"], data[\"meteor_scores\"], data.get(\"last_idx\", 0)\n",
    "        print(\"Resuming from index\", start_idx)\n",
    "    else:\n",
    "        refs, hyps, meteor_scores = [], [], []\n",
    "        start_idx = 0\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # iterate over dataset_split indices for reproducibility\n",
    "    for idx in tqdm(range(start_idx, len(dataset_split)), desc=\"Evaluating\", unit=\"sent\"):\n",
    "        ex = dataset_split[idx]\n",
    "        src_text = ex[\"src\"].lower()\n",
    "        ref_text = ex[\"tgt\"]\n",
    "        # build src tensor for one example (with padding/truncation to MAX_LEN)\n",
    "        src_ids = [BOS_EN] + sp_en.encode(src_text)[:MAX_LEN-2] + [EOS_EN]\n",
    "        src_ids += [PAD_EN] * (MAX_LEN - len(src_ids))\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        if use_beam:\n",
    "            pred_text = beam_decode(model, src_tensor, beam_size=BEAM_SIZE, max_len=MAX_GEN_LEN, length_penalty=LENGTH_PENALTY)\n",
    "        else:\n",
    "            pred_text = greedy_decode(model, src_tensor, max_len=MAX_GEN_LEN)\n",
    "\n",
    "        refs.append(ref_text)\n",
    "        hyps.append(pred_text)\n",
    "        meteor_scores.append(meteor_score([ref_text.split()], pred_text.split()))\n",
    "\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(dataset_split):\n",
    "            torch.save({\n",
    "                \"refs\": refs,\n",
    "                \"hyps\": hyps,\n",
    "                \"meteor_scores\": meteor_scores,\n",
    "                \"last_idx\": idx + 1\n",
    "            }, partial_path)\n",
    "            print(f\"Saved partial at sentence {idx+1}\")\n",
    "\n",
    "    # Compute corpus metrics\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
    "    ter_metric = sacrebleu.metrics.TER()\n",
    "    ter = ter_metric.corpus_score(hyps, [refs])\n",
    "    meteor_avg = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0\n",
    "\n",
    "    results = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"BLEU\": round(bleu.score, 2),\n",
    "        \"TER\": round(ter.score, 2),\n",
    "        \"METEOR\": round(meteor_avg * 100, 2)\n",
    "    }\n",
    "\n",
    "    torch.save(results, metrics_path)\n",
    "    if partial_path.exists():\n",
    "        try:\n",
    "            partial_path.unlink()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(\"Final metrics:\", results)\n",
    "    return results\n",
    "\n",
    "# =====================\n",
    "# Training loop (with validation + checkpointing)\n",
    "# =====================\n",
    "def train_loop(model, train_loader, val_loader, start_epoch=1, epochs=EPOCHS):\n",
    "    global best_val_loss, epochs_no_improve\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        model.train()\n",
    "        train_ce = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n",
    "        for src, tgt in pbar:\n",
    "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "            src_mask = create_padding_mask(src, 'en')\n",
    "            tgt_input, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(DEVICE)\n",
    "            tgt_key_padding_mask = create_padding_mask(tgt_input, 'hi')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type=\"cuda\" if DEVICE.startswith(\"cuda\") else \"cpu\"):\n",
    "                logits = model(\n",
    "                    src, tgt_input,\n",
    "                    src_key_padding_mask=src_mask,\n",
    "                    tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                    memory_key_padding_mask=src_mask,\n",
    "                    tgt_mask=tgt_mask\n",
    "                )  # (b, seq_len, vocab)\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            train_ce += loss.item()\n",
    "\n",
    "        avg_train_loss = train_ce / len(train_loader)\n",
    "        print(f\"Epoch {epoch} | Train CE Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "                src_mask = create_padding_mask(src, 'en')\n",
    "                tgt_input, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
    "                tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(DEVICE)\n",
    "                tgt_key_padding_mask = create_padding_mask(tgt_input, 'hi')\n",
    "\n",
    "                logits = model(\n",
    "                    src, tgt_input,\n",
    "                    src_key_padding_mask=src_mask,\n",
    "                    tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                    memory_key_padding_mask=src_mask,\n",
    "                    tgt_mask=tgt_mask\n",
    "                )\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"scaler_state_dict\": scaler.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"epochs_no_improve\": epochs_no_improve,\n",
    "        }, CHECKPOINT_PATH)\n",
    "\n",
    "        # Save best\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            print(\"New best model saved!\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "# =====================\n",
    "# Run training\n",
    "# =====================\n",
    "train_loop(model, train_loader, val_loader, start_epoch=start_epoch, epochs=EPOCHS)\n",
    "\n",
    "# Load best model for evaluation if exists\n",
    "if BEST_MODEL_PATH.exists():\n",
    "    print(\"Loading best model for evaluation:\", BEST_MODEL_PATH)\n",
    "    model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))\n",
    "else:\n",
    "    print(\"No best model found, using last model state.\")\n",
    "\n",
    "# =====================\n",
    "# Run evaluation (greedy and beam)\n",
    "# =====================\n",
    "# =====================\n",
    "# Run evaluation (greedy and beam)\n",
    "# =====================\n",
    "def print_metrics(title, path):\n",
    "    metrics = torch.load(path)\n",
    "    print(f\"\\n✅ {title} results ({path.name}):\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    return metrics\n",
    "\n",
    "# --- Greedy evaluation ---\n",
    "greedy_path = Path(\"metrics_gmsc_greedy.pt\")\n",
    "if greedy_path.exists():\n",
    "    print(\"Greedy metrics already exist — skipping evaluation.\")\n",
    "    metrics_greedy = print_metrics(\"Greedy Decode\", greedy_path)\n",
    "else:\n",
    "    print(\"Evaluating (greedy decode) on test split...\")\n",
    "    metrics_greedy = evaluate_and_save(model, test_data, use_beam=False, metrics_path=greedy_path)\n",
    "    print_metrics(\"Greedy Decode\", greedy_path)\n",
    "\n",
    "\n",
    "# Save sample translations\n",
    "sample_indices = random.sample(range(len(test_data)), 10)\n",
    "samples = []\n",
    "for idx in sample_indices:\n",
    "    ex = test_data[idx]\n",
    "    src_text = ex[\"src\"].lower()\n",
    "    src_ids = [BOS_EN] + sp_en.encode(src_text)[:MAX_LEN-2] + [EOS_EN]\n",
    "    src_ids += [PAD_EN] * (MAX_LEN - len(src_ids))\n",
    "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    pred_greedy = greedy_decode(model, src_tensor)\n",
    "    pred_beam = beam_decode(model, src_tensor)\n",
    "    samples.append({\n",
    "        \"src\": src_text,\n",
    "        \"ref\": ex[\"tgt\"],\n",
    "        \"pred_greedy\": pred_greedy,\n",
    "        \"pred_beam\": pred_beam\n",
    "    })\n",
    "\n",
    "with open(\"sample_translations_gmsc.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(samples, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved sample translations to sample_translations_gmsc.json\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMlhfeuGKoz9rXRdy80tODu",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (mt_env)",
   "language": "python",
   "name": "mt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
